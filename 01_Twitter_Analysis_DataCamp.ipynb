{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Streaming With Tweepy](http://docs.tweepy.org/en/latest/streaming_how_to.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tweepy) (1.7.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-13e08f521de3>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-13e08f521de3>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    def__init__(self, api = None):\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Slistener\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "\n",
    "class SListener(StreamListener):\n",
    "    def__init__(self, api = None):\n",
    "        self.output = open('tweets_%s.json %\n",
    "            time.strftime('%Y%m%d-%H%M%S'), 'w'')\n",
    "                           self.api or API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consumer_Key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2cf5d72ed437>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconsumer_Key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess_token_secret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'consumer_Key' is not defined"
     ]
    }
   ],
   "source": [
    "#tweepy authentication\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "auth = OAuthHandler(consumer_Key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting data with tweepy\n",
    "from tweepy import Stream\n",
    "listen = SListener(api)\n",
    "stream = Stream(auth, listen)\n",
    "stream.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SListener' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b5f8a32dbebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Instantiate the SListener object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlisten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Instantiate the Stream object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SListener' is not defined"
     ]
    }
   ],
   "source": [
    "# Collecting data on keywords\n",
    "from tweepy import Stream\n",
    "\n",
    "# Set up words to track\n",
    "keywords_to_track = ['#rstats', '#python']\n",
    "\n",
    "# Instantiate the SListener object \n",
    "listen = SListener(api)\n",
    "\n",
    "# Instantiate the Stream object\n",
    "stream = Stream(auth, listen)\n",
    "\n",
    "# Begin collecting data\n",
    "stream.filter(track = keywords_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Twitter JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of Twitter JSON\n",
    "{\n",
    "    \"created_at\": \"Thu Apr 19 14:25:04 +0000 2018\",\n",
    "    \"id\": 986973961295720449,\n",
    "    \"id_str\": \"986973961295720449\",\n",
    "    \"text\": \"Writing out the script of my @DataCamp class\n",
    "        and I can't help but mentally read it back to myself in\n",
    "        @hugobownes voice.\",\n",
    "    \"retweet_count\": 0,\n",
    "    \"favorite_count\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"user\": {\n",
    "        \"id\": 661613,\n",
    "        \"name\":\"Alex Hanna\", \"Data Witch\",\n",
    "        \"screen_name\": \"alexhanna\",\n",
    "        \"location\": \"Toronto, ON\",\n",
    "        ...\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tweet_json = open('tweet-example.json', 'r').read()\n",
    "tweet = json.loads(tweet_json)\n",
    "tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child tweet JSON\n",
    "tweet['user']['screen_name']\n",
    "tweet['user']['name']\n",
    "twet['user']['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "import json\n",
    "\n",
    "# Convert from JSON to Python object\n",
    "tweet = json.loads(tweet_json)\n",
    "\n",
    "# Print tweet text\n",
    "print(tweet['text'])\n",
    "\n",
    "# Print tweet id\n",
    "print(tweet['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user handle\n",
    "print(tweet['user']['screen_name'])\n",
    "\n",
    "# Print user follower count\n",
    "print(tweet['user']['followers_count'])\n",
    "\n",
    "# Print user location\n",
    "print(tweet['user']['location'])\n",
    "\n",
    "# Print user description\n",
    "print(tweet['user']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e0c170990d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print the text of the tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Print the text of tweet which has been retweeted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'retweeted_status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rt' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the text of the tweet\n",
    "print(rt['text'])\n",
    "\n",
    "# Print the text of tweet which has been retweeted\n",
    "print(rt['retweeted_status']['text'])\n",
    "\n",
    "# Print the user handle of the tweet\n",
    "print(rt['user']['screen_name'])\n",
    "\n",
    "# Print the user handle of the tweet which has been retweeted\n",
    "print(rt['retweeted_status']['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text in Twitter JSON\n",
    "tweet_json = open('tweet-example.json', 'r').read()\n",
    "tweet = json.loads(tweet_json)\n",
    "tweet['text']\n",
    "tweet['extended_tweet']['full_text']\n",
    "tweet['quoted_status']['extended_tweet']['full_text']\n",
    "\n",
    "# Text user information\n",
    "tweet['user']['descrition']\n",
    "tweet['user']['location']\n",
    "\n",
    "#Flattening Twitter JSON\n",
    "extended_tweet['extended_tweet-ull_text'] =\n",
    "    extended_tweet['extended_tweet']['full_text']\n",
    "    \n",
    "tweet_list = []\n",
    "with open('all_tweets.json', 'r') as fh:\n",
    "    tweets_json = fh.read().split('\\n')\n",
    "    \n",
    "    for tweet in tweets_json:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "        \n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            tweet_obj['extended_tweet-full_text'] =\n",
    "                tweet_obj['extendend_tweet']['full_text']\n",
    "        ...\n",
    "    tweet_list.append(tweet)\n",
    "    \n",
    "tweets = pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tweet text\n",
    "print(quoted_tweet['text'])\n",
    "\n",
    "# Print the quoted tweet text\n",
    "print(quoted_tweet['quoted_status']['text'])\n",
    "\n",
    "# Print the quoted tweet's extended (140+) text\n",
    "print(quoted_tweet['quoted_status']['extended_tweet']['full_text'])\n",
    "\n",
    "# Print the quoted user location\n",
    "print(quoted_tweet['quoted_status']['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the user screen_name in 'user-screen_name'\n",
    "quoted_tweet['user-screen_name'] = quoted_tweet['user']['screen_name']\n",
    "\n",
    "# Store the quoted_status text in 'quoted_status-text'\n",
    "quoted_tweet['quoted_status-text'] = quoted_tweet['quoted_status']['text']\n",
    "\n",
    "# Store the quoted tweet's extended (140+) text in \n",
    "# 'quoted_status-extended_tweet-full_text'\n",
    "quoted_tweet['quoted_status-extended_tweet-full_text'] = quoted_tweet['quoted_status']['extended_tweet']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tweet flattening function\n",
    "def flatten_tweets(tweets_json):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON\n",
    "        is in a top-level dictionary.\"\"\"\n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet in tweets_json:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "    \n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "    \n",
    "        # Check if this is a 140+ character tweet\n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            # Store the extended tweet text in 'extended_tweet-full_text'\n",
    "            tweet_obj['extended_tweet-full_text'] = tweet_obj['extended_tweet']['full_text']\n",
    "    \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in 'retweeted_status-user-screen_name'\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']\n",
    "            \n",
    "        tweets_list.append(tweet_obj)\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tweets into a DataFrame\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten the tweets and store in `tweets`\n",
    "tweets = flatten_tweets(data_science_json)\n",
    "\n",
    "# Create a DataFrame from `tweets`\n",
    "ds_tweets = pd.DataFrame(tweets)\n",
    "\n",
    "# Print out the first 5 tweets from this dataset\n",
    "print(ds_tweets['text'].values[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting words\n",
    "import pandas as pd\n",
    "tweets = pd.DataFrame(flatten_tweets(companies_json))\n",
    "apple = tweets['text'].str.contains('apple', case = False)\n",
    "print(np.sum(apple) / tweets.shape[0])\n",
    "\n",
    "# Counting in multiple text fields\n",
    "apple = tweets['text'].str.contains('apple', case = False)\n",
    "\n",
    "for column in ['extended_tweet-full_text',\n",
    "  'retweeted_status-text',\n",
    "  'retweeted_status-extended_tweet-full_text']:\n",
    " apple = apple | tweets[column].str.contains('apple', case = False)\n",
    "\n",
    "print(np.sum(apple) / tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding keywords\n",
    "# Flatten the tweets and store them\n",
    "flat_tweets = flatten_tweets(data_science_json)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ds_tweets = pd.DataFrame(flat_tweets)\n",
    "\n",
    "# Find mentions of #python in 'text'\n",
    "python = ds_tweets['text'].str.contains('#python', case = False)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for text in all the wrong places\n",
    "def check_word_in_tweet(word, data):\n",
    "    \"\"\"Checks if a word is in a Twitter dataset's text. \n",
    "    Checks text and extended tweet (140+ character tweets) for tweets,\n",
    "    retweets and quoted tweets.\n",
    "    Returns a logical pandas Series.\n",
    "    \"\"\"\n",
    "    contains_column = data['text'].str.contains(word, case = False)\n",
    "    contains_column |= data['extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    contains_column |= data['quoted_status-text'].str.contains(word, case = False) \n",
    "    contains_column |= data['quoted_status-extended_tweet-full_text'].str.contains(word, case = False) \n",
    "    contains_column |= data['retweeted_status-text'].str.contains(word, case = False) \n",
    "    contains_column |= data['retweeted_status-extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    return contains_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing #python to #rstats\n",
    "# Find mentions of #python in all text fields\n",
    "python = check_word_in_tweet(\"#python\", ds_tweets)\n",
    "\n",
    "# Find mentions of #rstats in all text fields\n",
    "rstats = check_word_in_tweet(\"#rstats\", ds_tweets)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])\n",
    "\n",
    "# Print proportion of tweets mentioning #rstats\n",
    "print(\"Proportion of #rstats tweets:\", np.sum(rstats) / ds_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series\n",
    "print(tweets['created_at'])\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])\n",
    "print(tweets['created_at'])\n",
    "tweets = tweets.set_index('created_at')\n",
    "\n",
    "# Keywords as time series metrics\n",
    "tweets['google'] = check_word_in_tweet('google', tweets)\n",
    "print(tweets['google'])\n",
    "\n",
    "print(np.sum(tweets['google']))\n",
    "\n",
    "# Generating keyword means\n",
    "mean_google = tweets['google'].resample('1 min').mean()\n",
    "print(mean_google)\n",
    "\n",
    "# Plotting keyword means\n",
    "import matplolib.pyplot as plt\n",
    "\n",
    "plt.plot(means_facebook.index.minute, means_facebook, color = 'blue')\n",
    "plt.plot(means_google.index.minute, means-google, color = 'grren')\n",
    "plt.xlabel('Minute')\n",
    "plt.title('Company mentions')\n",
    "plt.legend(('facebook', 'google'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time series data frame\n",
    "# Print created_at to see the original format of datetime in Twitter data\n",
    "print(ds_tweets['created_at'].head())\n",
    "\n",
    "# Convert the created_at column to np.datetime object\n",
    "ds_tweets['created_at'] = pd.to_datetime(ds_tweets['created_at'])\n",
    "\n",
    "# Print created_at to see new format\n",
    "print(ds_tweets['created_at'].head())\n",
    "\n",
    "# Set the index of ds_tweets to created_at\n",
    "ds_tweets = ds_tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating mean frequency\n",
    "# Create a python column\n",
    "ds_tweets['python'] = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Create an rstats column\n",
    "ds_tweets['rstats'] = check_word_in_tweet('#rstats', ds_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean frequency\n",
    "# Average of python column by day\n",
    "mean_python = ds_tweets['python'].resample('1 d').mean()\n",
    "\n",
    "# Average of rstats column by day\n",
    "mean_rstats = ds_tweets['rstats'].resample('1 d').mean()\n",
    "\n",
    "# Plot mean python/rstats by day\n",
    "plt.plot(mean_python.index.day, mean_python, color = 'green')\n",
    "plt.plot(mean_rstats.index.day, mean_rstats, color = 'blue')\n",
    "\n",
    "# Add labels and show\n",
    "plt.xlabel('Day'); plt.ylabel('Frequency')\n",
    "plt.title('Language mentions over time')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = tweets['text'].apply(sid.polarity_scores)\n",
    "\n",
    "# Interpreting sentiment scores\n",
    "tweet1 = 'RT @jeffrey_heer: Thanks for inviting me, and thanks for the lovely visualization of the talk! ...'\n",
    "print(sid.polarity_scores(tweet1))\n",
    "{'neg':0.0, 'neu': 0.496, 'pos': 0.504, 'compound': 0.9041}\n",
    "\n",
    "tweet2 = 'i am having problemns with google play music'\n",
    "print(sid.polarity_scores(tweet2))\n",
    "{'neg': 0.267, 'neu': 0.495, 'pos':0.238, 'compound': -0.0772}\n",
    "\n",
    "# Generating sentiment averages\n",
    "sentiment = sentiment_scores.apply(lambda x: x['compound'])\n",
    "sentiment_fb = sentiment[check_word_in_tweet('facebook', tweets)].resample('1 min').mean()\n",
    "sentiment_gg = sentiment[check_word_in_tweet('google', tweets)].resample('1 min').mean()\n",
    "\n",
    "# Plotting sentiment scores\n",
    "plt.plot(sentiment_fb.index.minute, sentiment_fb, color = 'blue')\n",
    "plt.plot(sentiment_gg.index.minute, sentiment_gg, color = 'green')\n",
    "\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment of companies')\n",
    "plt.legend(('Facebook', 'Google'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading VADER\n",
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = ds_tweets['text'].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiment scores\n",
    "# Print out the text of a positive tweet\n",
    "print(ds_tweets[sentiment > 0.6]['text'].values[0])\n",
    "\n",
    "# Print out the text of a negative tweet\n",
    "print(ds_tweets[sentiment < -0.6]['text'].values[0])\n",
    "\n",
    "# Generate average sentiment scores for #python\n",
    "sentiment_py = sentiment[ check_word_in_tweet('#python', ds_tweets) ].resample('1 d').mean()\n",
    "\n",
    "# Generate average sentiment scores for #rstats\n",
    "sentiment_r = sentiment[ check_word_in_tweet('#rstats', ds_tweets) ].resample('1 d').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sentiment scores\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average #python sentiment per day\n",
    "plt.plot(sentiment_py.index.day, sentiment_py, color = 'green')\n",
    "\n",
    "# Plot average #rstats sentiment per day\n",
    "plt.plot(sentiment_r.index.day, sentiment_r, color = 'blue')\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment of data science languages')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and visualization Tweest Networks\n",
    "import networkx as nx\n",
    "## ... flatten and covert JSON\n",
    "G_rt = nx.from_pandas_edgelist(tweets, source = 'user-screen_name', target = 'retweeted_status-user-screen_name', \n",
    "                               create_using = nx.DiGraph())\n",
    "\n",
    "# Importing a quoted network\n",
    "import networkx as nx\n",
    "## ... flatten and covert JSON\n",
    "G_reply = nx.from_pandas_edgelist(tweets, source = 'user-screen_name', target = 'in_reply_to_screen_name', \n",
    "                                  create_using = nx.DiGraph())\n",
    "\n",
    "# Visualization\n",
    "x.draw_networkx(T)\n",
    "plt.axis('off')\n",
    "\n",
    "sizes =\n",
    "    [x[1]*100 for x in T.degree()]\n",
    "nx.draw_network(T, node_size = sizes, with_labels = False, alpha = 0.6, width = 0.3)\n",
    "plt.axis('off')\n",
    "\n",
    "# Circular layout\n",
    "circle_pos = nx.circular_layout(T)\n",
    "nx.draw_networkx(T, pos = circle_pos, node_size = sizes, with_labels = False, alpha = 0.6, width = 0.3)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating retweet network\n",
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Create retweet network from edgelist\n",
    "G_rt = nx.from_pandas_edgelist(\n",
    "    sotu_retweets,\n",
    "    source = 'user-screen_name',\n",
    "    target = 'retweeted_status-user-screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    " \n",
    "# Print the number of nodes\n",
    "print('Nodes in RT network:', len(G_rt.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in RT network:', len(G_rt.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating reply network\n",
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Create reply network from edgelist\n",
    "G_reply = nx.from_pandas_edgelist(\n",
    "    sotu_replies,\n",
    "    source = 'user-screen_name',\n",
    "    target = 'in_reply_to_screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    "    \n",
    "# Print the number of nodes\n",
    "print('Nodes in reply network:', len(G_reply.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in reply network:', len(G_reply.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing retweet network\n",
    "# Create random layout positions\n",
    "pos = nx.random_layout(G_rt)\n",
    "\n",
    "# Create size list\n",
    "sizes = [x[1] for x in G_rt.degree()]\n",
    "\n",
    "# Draw the network\n",
    "nx.draw_networkx(G_rt, pos, \n",
    "    with_labels = False, \n",
    "    node_size = sizes,\n",
    "    width = 0.1, alpha = 0.7,\n",
    "    arrowsize = 2, linewidths = 0)\n",
    "\n",
    "# Turn axis off and show\n",
    "plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: node importance\n",
    "# Degree centrality\n",
    "nx.in_degree_centrality(T)\n",
    "nx.out_degree_centrality(T)\n",
    "\n",
    "# Betweenness Centrality\n",
    "nx.betweenness_centrality(T)\n",
    "\n",
    "# Priting highest centrality\n",
    "bc = nx.betweenness_centrality(T)\n",
    "\n",
    "betweenness = pd.DataFrame(list(bc.items()), list(bc.items()), columns = ['Name', 'Cent'])\n",
    "print(betweenness.sort_values('Cent', ascending = False).head())\n",
    "\n",
    "#********************************************************************************************************************#\n",
    "#                                           Centrality in different networks                                         #\n",
    "# ------------------------------------------------------------------------------------------------------------------ #\n",
    "#                                                                 Centrality                                         #\n",
    "#                        ___________________________________________________________________________________________ #\n",
    "#                            In-Degree          Out-Degree                          Betweeness                       #\n",
    "# __________________________________________________________________________________________________________________ #\n",
    "#          * Retweets  *  Gets retweets   *  Shares retweets     *   Bridges different topic/ideology communities    #\n",
    "# Network  * _______________________________________________________________________________________________________ #\n",
    "# Type     * Replies   *  Gets most       *  Participates in     *   Bridges different topic/ideology discussions    #\n",
    "#          *           *  replies         *  many conversations  *                                                   #\n",
    "#********************************************************************************************************************#\n",
    "\n",
    "# The Ratio\n",
    "degree_rt = pd.DataFrame(list(G_rt.in_degree()), columns = ['screen_name', 'degree'])\n",
    "degree_reply = pd.DataFrame(list(G_reply.in_degree()), columns = ['screen_name', 'degree'])\n",
    "\n",
    "ratio = degree_rt.merge(degree_reply, on = 'screen_name', suffixes = ('_rt', '_reply'))\n",
    "ratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-degree centrality\n",
    "# Generate in-degree centrality for retweets \n",
    "rt_centrality = nx.in_degree_centrality(G_rt)\n",
    "\n",
    "# Generate in-degree centrality for replies \n",
    "reply_centrality = nx.in_degree_centrality(G_reply)\n",
    "\n",
    "# Store centralities in DataFrame\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('degree_centrality', ascending = False).head())\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(reply.sort_values('degree_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness Centrality\n",
    "# Generate betweenness centrality for retweets \n",
    "rt_centrality = nx.betweenness_centrality(G_rt)\n",
    "\n",
    "# Generate betweenness centrality for replies \n",
    "reply_centrality = nx.betweenness_centrality(G_reply)\n",
    "\n",
    "# Store centralities in data frames\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('betweenness_centrality', ascending = False).head())\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(reply.sort_values('betweenness_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios\n",
    "# Calculate in-degrees and store in DataFrame\n",
    "degree_rt = pd.DataFrame(list(G_rt.in_degree()), columns = column_names)\n",
    "degree_reply = pd.DataFrame(list(G_reply.in_degree()), columns = column_names)\n",
    "\n",
    "# Merge the two DataFrames on screen name\n",
    "ratio = degree_rt.merge(degree_reply, on = 'screen_name', suffixes = ('_rt', '_reply'))\n",
    "\n",
    "# Calculate the ratio\n",
    "ratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']\n",
    "\n",
    "# Exclude any tweets with less than 5 retweets\n",
    "ratio = ratio[ratio['degree_rt'] >= 5]\n",
    "\n",
    "# Print out first five with highest ratio\n",
    "print(ratio.sort_values('ratio', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps and Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographical Data in Twitter JSON\n",
    "print(tweet['place'])\n",
    "\n",
    "{'attributes': {},\n",
    "    'bounding_box': {'coordinates':[[[-80.47611, 37.185195],\n",
    "                                     [-80.47611, 37.273387],\n",
    "                                     [-80.381618, 37.273387],\n",
    "                                     [-80.381618, 37.185195]]],\n",
    "                    'type': 'Polygon'},\n",
    "                    'country': 'United States',\n",
    "                    'country_code': 'US',\n",
    "                    'full_name': 'Blacksburg',\n",
    "                    'place_type': 'city',\n",
    "                    ...}\n",
    "\n",
    "# Calculating the centroid\n",
    "coordinates = [\n",
    "    [-80.47611, 37.185195],\n",
    "    [-80.47611, 37.273387],\n",
    "    [-80.381618, 37.273387],\n",
    "    [-80.381618, 37.185195]]\n",
    "\n",
    "longs = np.unique( [x[0] for x in coordinates] )\n",
    "lats = np.unique( x[1] for x in coordinates] )\n",
    "\n",
    "central_long = np.sum(longs) / 2\n",
    "central_lat = np.sum(lats) / 2\n",
    "\n",
    "# Coordinates JSON\n",
    "print(tweet['coordinates'])\n",
    "\n",
    "{'type': 'Point',\n",
    "'coordintates': [-72.2833, 21.7833]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing user-defined location\n",
    "# Print out the location of a single tweet\n",
    "print(tweet_json['user']['location'])\n",
    "\n",
    "# Flatten and load the SOTU tweets into a dataframe\n",
    "tweets_sotu = pd.DataFrame(flatten_tweets(tweets_sotu_json))\n",
    "\n",
    "# Print out top five user-defined locations\n",
    "print(tweets_sotu['user-location'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing bounding box\n",
    "def getBoundingBox(place):\n",
    "    \"\"\" Returns the bounding box coordinates.\"\"\"\n",
    "    return place['bounding_box']['coordinates']\n",
    "\n",
    "# Apply the function which gets bounding box coordinates\n",
    "bounding_boxes = tweets_sotu['place'].apply(getBoundingBox)\n",
    "\n",
    "# Print out the first bounding box coordinates\n",
    "print(bounding_boxes.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the centroid\n",
    "def calculateCentroid(place):\n",
    "    \"\"\" Calculates the centroid from a bounding box.\"\"\"\n",
    "    # Obtain the coordinates from the bounding box.\n",
    "    coordinates = place['bounding_box']['coordinates'][0]\n",
    "        \n",
    "    longs = np.unique( [x[0] for x in coordinates] )\n",
    "    lats  = np.unique( [x[1] for x in coordinates] )\n",
    "\n",
    "    if len(longs) == 1 and len(lats) == 1:\n",
    "        # return a single coordinate\n",
    "        return (longs[0], lats[0])\n",
    "    elif len(longs) == 2 and len(lats) == 2:\n",
    "        # If we have two longs and lats, we have a box.\n",
    "        central_long = np.sum(longs) / 2\n",
    "        central_lat  = np.sum(lats) / 2\n",
    "    else:\n",
    "        raise ValueError(\"Non-rectangular polygon not supported: %s\" % \n",
    "            \",\".join(map(lambda x: str(x), coordinates)) )\n",
    "\n",
    "    return (central_long, central_lat)\n",
    "    \n",
    "# Calculate the centroids of place     \n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Twitter maps\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "m = Basemap(projection = 'merc', llcrnrlat = -35,62, llcrnrlon = -17,29, urcrnrlat = 37.73, urcrnrlon = 51.39)\n",
    "\n",
    "m.fillcontinentes(color = 'white')\n",
    "m.drawcoastlines(color = 'gray')\n",
    "m.drawcountries(color = 'gray')\n",
    "\n",
    "# Plotting points\n",
    "africa = pd.read_cs('africa.csv')\n",
    "longs = africa['CapitalLongtitude']\n",
    "lats = africa['CapitalLatitude']\n",
    "\n",
    "m = Basemap(...)\n",
    "\n",
    "m.fillcontinents(color = 'white',zorder = 0)\n",
    "m.drawcoastlines(color = 'gray')\n",
    "m.drawcountries(color = 'gray')\n",
    "\n",
    "m.scatter(longs.values, lats.values, latlon = True, alpha = 0.7)\n",
    "\n",
    "# Using color\n",
    "m.scatter(longs.values, lats.values, latlon = True, c = arabic.values, cmap = 'Paired' alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Basemap map\n",
    "# Import Basemap\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the US bounding box\n",
    "us_boundingbox = [-125, 22, -64, 50] \n",
    "\n",
    "# Set up the Basemap object\n",
    "m = Basemap(llcrnrlon = us_boundingbox[0],\n",
    "            llcrnrlat = us_boundingbox[1],\n",
    "            urcrnrlon = us_boundingbox[2],\n",
    "            urcrnrlat = us_boundingbox[3],\n",
    "            projection='merc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Basemap map\n",
    "# Import Basemap\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the US bounding box\n",
    "us_boundingbox = [-125, 22, -64, 50] \n",
    "\n",
    "# Set up the Basemap object\n",
    "m = Basemap(llcrnrlon = us_boundingbox[0],\n",
    "            llcrnrlat = us_boundingbox[1],\n",
    "            urcrnrlon = us_boundingbox[2],\n",
    "            urcrnrlat = us_boundingbox[3],\n",
    "            projection='merc')\n",
    "\n",
    "# Draw continents in white,\n",
    "# coastlines and countries in gray\n",
    "m.fillcontinents(color='white')\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "\n",
    "# Draw the states and show the plot\n",
    "m.drawstates(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting centroid coordinates\n",
    "# Calculate the centroids for the dataset \n",
    "# and isolate longitudue and latitudes\n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)\n",
    "lon = [x[0] for x in centroids]\n",
    "lat = [x[1] for x in centroids]\n",
    "\n",
    "# Draw continents, coastlines, countries, and states\n",
    "m.fillcontinents(color='white', zorder = 0)\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "m.drawstates(color='gray')\n",
    "\n",
    "# Draw the points and show the plot\n",
    "m.scatter(lon, lat, latlon = True, alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring by sentiment\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = tweets_sotu['text'].apply(sid.polarity_scores)\n",
    "\n",
    "# Isolate the compound element\n",
    "sentiment_scores = [x['compound'] for x in sentiment_scores]\n",
    "\n",
    "# Draw the points\n",
    "m.scatter(lon, lat, latlon = True, \n",
    "           c = sentiment_scores,\n",
    "           cmap = 'coolwarm', alpha = 0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
